{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our Final Project in the course Social Data Analysis at DTU we decided to analyse the data-set of all taxi trips in 2015 in New York City. \n",
    "Our initial study is centred on Taxi trends throughout the year and the day, but we will utilise the information obtained to make a comparison with new transport providers like Uber and to visually see the impact of the recent Covid-Pandemic on life in big cities. \n",
    "We believe this data-set provides endless opportunities and that we have only scratched the surface with our studies. However some our visualisations show how fascinating the data-set is and how with data we comprehend the world’s dynamics. \n",
    "\n",
    "\n",
    "Our Analysis is guided by a Consistent Visual Platform which we use to direct the path of interest of the User, providing multiple articles as well as summaries and annotations. \n",
    "We also decided to realise many of our graphs in bokeh form in order to deliver a more stimulating experience for the whomever is interested in visualising our work, even if it is not their area of expertise. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic stats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is structured in a total of 17 columns containing information about single taxi trips in NYC. \n",
    "\n",
    "- vendorid: A code indicating the TPEP provider that provided the record.\n",
    "\n",
    "  1= Creative Mobile Technologies, LLC; 2= VeriFone Inc.\n",
    "  \n",
    "  \n",
    "- tpep_pickup_datetime: The date and time when the meter was engaged. \n",
    "\n",
    "- tpep_dropoff_datetime: The date and time when the meter was disengaged. \n",
    "\n",
    "- passenger_count: The number of passengers in the vehicle. This is a driver-entered value.\n",
    "\n",
    "- trip_distance: The elapsed trip distance in miles reported by the taximeter\n",
    "\n",
    "- pulocationid: TLC Taxi Zone in which the taximeter was engaged\n",
    "\n",
    "- dolocationid: TLC Taxi Zone in which the taximeter was disengaged\n",
    "\n",
    "- ratecodeid: The final rate code in effect at the end of the trip.\n",
    "\n",
    "    1= Standard rate, \n",
    "    2= JFK, \n",
    "    3= Newark, \n",
    "    4= Nassau or Westchester, \n",
    "    5= Negotiated fare, \n",
    "    6= Group ride\n",
    "    \n",
    "- store_and_fwd_flag: This flag indicates whether the trip record was held in vehicle\n",
    "    memory before sending to the vendor, aka “store and forward,”\n",
    "    because the vehicle did not have a connection to the server.\n",
    "\n",
    "    Y= store and forward trip, \n",
    "    N= not a store and forward trip\n",
    "    \n",
    "- payment_type: A numeric code signifying how the passenger paid for the trip.\n",
    "\n",
    "    1= Credit card, \n",
    "    2= Cash, \n",
    "    3= No charge, \n",
    "    4= Dispute, \n",
    "    5= Unknown, \n",
    "    6= Voided trip\n",
    "- fare_amount: The time-and-distance fare calculated by the meter.\n",
    "\n",
    "- extra: Miscellaneous extras and surcharges. Currently, this only includes the 0.50 dollars and 1 dollar rush hour and overnight charges.\n",
    "\n",
    "- mta_tax: 0.50 dollars MTA tax that is automatically triggered based on the metered\n",
    "    rate in use.\n",
    "  \n",
    "- improvement_surcharge: 0.30 dollars improvement surcharge assessed trips at the flag drop. The\n",
    "    improvement surcharge began being levied in 2015.\n",
    "\n",
    "- tip_amount: This field is automatically populated for credit card\n",
    "    tips. Cash tips are not included.\n",
    "    \n",
    "- tolls_amount: Total amount of all tolls paid in trip. \n",
    "\n",
    "- total_amount: The total amount charged to passengers. Does not include cash tips.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next part it will be describe part by part with the same chronology that it was used in the webpage. Since, is how we started the notebook and is a more natural way to follow it. So that, the reviewer can see how we started to add more and more analysis after findings in the previous parts or solving different problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, the needed libraries are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sodapy import Socrata\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import random\n",
    "import os\n",
    "import folium \n",
    "from folium.plugins import *\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#!/usr/bin/env python\n",
    "\n",
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.cityofnewyork.us\", None)\n",
    "\n",
    "# Example authenticated client (needed for non-public datasets):\n",
    "client = Socrata(\"data.cityofnewyork.us\",\n",
    "                 \"Hi16i3SK4y2UNiONqpKz4IROh\",\n",
    "                 username=\"****************\",\n",
    "                 password=\"****************,\n",
    "                 timeout=120)\n",
    "#https://data.cityofnewyork.us/resource/gkne-dk5s.json\n",
    "# First 2000 results, returned as JSON from API / converted to Python list of\n",
    "# dictionaries by sodapy.\n",
    "results = client.get(\"f9tw-8p66\", limit=171000) #Uber rows without NaN\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_taxi_2009 = pd.DataFrame.from_records(results)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#!/usr/bin/env python\n",
    "\n",
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.cityofnewyork.us\", None)\n",
    "\n",
    "# Example authenticated client (needed for non-public datasets):\n",
    "client = Socrata(\"data.cityofnewyork.us\",\n",
    "                 \"Hi16i3SK4y2UNiONqpKz4IROh\",\n",
    "                 username=\"****************\",\n",
    "                 password=\"****************,\n",
    "                 timeout=120)\n",
    "#https://data.cityofnewyork.us/resource/gkne-dk5s.json\n",
    "# First 2000 results, returned as JSON from API / converted to Python list of\n",
    "# dictionaries by sodapy.\n",
    "results = client.get(\"uwyp-dntv\", limit=173000) #Uber rows without NaN\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_taxi_2011 = pd.DataFrame.from_records(results)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#!/usr/bin/env python\n",
    "\n",
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.cityofnewyork.us\", None)\n",
    "\n",
    "# Example authenticated client (needed for non-public datasets):\n",
    "client = Socrata(\"data.cityofnewyork.us\",\n",
    "                 \"Hi16i3SK4y2UNiONqpKz4IROh\",\n",
    "                 username=\"****************\",\n",
    "                 password=\"****************,\n",
    "                 timeout=120)\n",
    "#https://data.cityofnewyork.us/resource/gkne-dk5s.json\n",
    "# First 2000 results, returned as JSON from API / converted to Python list of\n",
    "# dictionaries by sodapy.\n",
    "results = client.get(\"kerk-3eby\", limit=176000) #Uber rows without NaN\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_taxi_2012 = pd.DataFrame.from_records(results)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#!/usr/bin/env python\n",
    "\n",
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.cityofnewyork.us\", None)\n",
    "\n",
    "# Example authenticated client (needed for non-public datasets):\n",
    "client = Socrata(\"data.cityofnewyork.us\",\n",
    "                 \"Hi16i3SK4y2UNiONqpKz4IROh\",\n",
    "                 username=\"****************\",\n",
    "                 password=\"****************,\n",
    "                 timeout=120)\n",
    "#https://data.cityofnewyork.us/resource/gkne-dk5s.json\n",
    "# First 2000 results, returned as JSON from API / converted to Python list of\n",
    "# dictionaries by sodapy.\n",
    "results = client.get(\"t7ny-aygi\", limit=173000) #Uber rows without NaN\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_taxi_2013 = pd.DataFrame.from_records(results)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.cityofnewyork.us\", None)\n",
    "\n",
    "# Example authenticated client (needed for non-public datasets):\n",
    "client = Socrata(\"data.cityofnewyork.us\",\n",
    "                 \"Hi16i3SK4y2UNiONqpKz4IROh\",\n",
    "                 username=\"*****************\",\n",
    "                 password=\"***************\",\n",
    "                 timeout=120)\n",
    "#https://data.cityofnewyork.us/resource/gkne-dk5s.json\n",
    "# First 2000 results, returned as JSON from API / converted to Python list of\n",
    "# dictionaries by sodapy.\n",
    "results = client.get(\"gkne-dk5s\", limit=3000000) #Uber rows without NaN\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_taxi= pd.DataFrame.from_records(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#!/usr/bin/env python\n",
    "\n",
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.cityofnewyork.us\", None)\n",
    "\n",
    "# Example authenticated client (needed for non-public datasets):\n",
    "client = Socrata(\"data.cityofnewyork.us\",\n",
    "                 \"Hi16i3SK4y2UNiONqpKz4IROh\",\n",
    "                 username=\"****************\",\n",
    "                 password=\"****************,\n",
    "                 timeout=120)\n",
    "#https://data.cityofnewyork.us/resource/gkne-dk5s.json\n",
    "# First 2000 results, returned as JSON from API / converted to Python list of\n",
    "# dictionaries by sodapy.\n",
    "results = client.get(\"2yzn-sicd\", limit=146000) #Uber rows without NaN\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_taxi_2015 = pd.DataFrame.from_records(results)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#!/usr/bin/env python\n",
    "\n",
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.cityofnewyork.us\", None)\n",
    "\n",
    "# Example authenticated client (needed for non-public datasets):\n",
    "client = Socrata(\"data.cityofnewyork.us\",\n",
    "                 \"Hi16i3SK4y2UNiONqpKz4IROh\",\n",
    "                 username=\"****************\",\n",
    "                 password=\"****************,\n",
    "                 timeout=120)\n",
    "#https://data.cityofnewyork.us/resource/gkne-dk5s.json\n",
    "# First 2000 results, returned as JSON from API / converted to Python list of\n",
    "# dictionaries by sodapy.\n",
    "results = client.get(\"uacg-pexx\", limit=131000) #Uber rows without NaN\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_taxi_2016 = pd.DataFrame.from_records(results)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#!/usr/bin/env python\n",
    "\n",
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.cityofnewyork.us\", None)\n",
    "\n",
    "# Example authenticated client (needed for non-public datasets):\n",
    "client = Socrata(\"data.cityofnewyork.us\",\n",
    "                 \"Hi16i3SK4y2UNiONqpKz4IROh\",\n",
    "                 username=\"****************\",\n",
    "                 password=\"****************,\n",
    "                 timeout=120)\n",
    "#https://data.cityofnewyork.us/resource/gkne-dk5s.json\n",
    "# First 2000 results, returned as JSON from API / converted to Python list of\n",
    "# dictionaries by sodapy.\n",
    "results = client.get(\"biws-g3hs\", limit=11000000) #Uber rows without NaN\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_taxi_2017 = pd.DataFrame.from_records(results)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#!/usr/bin/env python\n",
    "\n",
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.cityofnewyork.us\", None)\n",
    "\n",
    "# Example authenticated client (needed for non-public datasets):\n",
    "client = Socrata(\"data.cityofnewyork.us\",\n",
    "                 \"Hi16i3SK4y2UNiONqpKz4IROh\",\n",
    "                 username=\"****************\",\n",
    "                 password=\"****************,\n",
    "                 timeout=120)\n",
    "#https://data.cityofnewyork.us/resource/gkne-dk5s.json\n",
    "# First 2000 results, returned as JSON from API / converted to Python list of\n",
    "# dictionaries by sodapy.\n",
    "results = client.get(\"t29m-gskq\", limit=112000) #Uber rows without NaN\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_taxi_2018 = pd.DataFrame.from_records(results)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#!/usr/bin/env python\n",
    "\n",
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.cityofnewyork.us\", None)\n",
    "\n",
    "# Example authenticated client (needed for non-public datasets):\n",
    "client = Socrata(\"data.cityofnewyork.us\",\n",
    "                 \"Hi16i3SK4y2UNiONqpKz4IROh\",\n",
    "                 username=\"****************\",\n",
    "                 password=\"****************,\n",
    "                 timeout=120)\n",
    "#https://data.cityofnewyork.us/resource/gkne-dk5s.json\n",
    "# First 2000 results, returned as JSON from API / converted to Python list of\n",
    "# dictionaries by sodapy.\n",
    "results = client.get(\"2upf-qytp\", limit=84000) #Uber rows without NaN\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_taxi_2019 = pd.DataFrame.from_records(results)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''#!/usr/bin/env python\n",
    "\n",
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.cityofnewyork.us\", None)\n",
    "\n",
    "# Example authenticated client (needed for non-public datasets):\n",
    "client = Socrata(\"data.cityofnewyork.us\",\n",
    "                 \"Hi16i3SK4y2UNiONqpKz4IROh\",\n",
    "                 username=\"****************\",\n",
    "                 password=\"****************,\n",
    "                 timeout=120)\n",
    "#https://data.cityofnewyork.us/resource/gkne-dk5s.json\n",
    "# First 2000 results, returned as JSON from API / converted to Python list of\n",
    "# dictionaries by sodapy.\n",
    "results = client.get(\"kxp8-n2sj\", limit=24000) #Uber rows without NaN\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_taxi_2020 = pd.DataFrame.from_records(results)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was merged the different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df_taxi=(df_taxi_2020\n",
    "         .append(df_taxi_2019)\n",
    "         .append(df_taxi_2018)\n",
    "         .append(df_taxi_2017)\n",
    "         .append(df_taxi_2016))\n",
    "df_taxi.rename(columns = {'pulocationid': 'pickup_location',\n",
    "                          'dolocationid': 'dropoff_location'\n",
    "                         }, inplace = True)\n",
    "\n",
    "df_taxi_2011.rename(columns = {'dropoff_datetime': 'tpep_dropoff_datetime'}, inplace = True)\n",
    "\n",
    "df_taxi_2014.rename(columns = {'pickup_datetime': 'tpep_pickup_datetime',\n",
    "                               'dropoff_datetime': 'tpep_dropoff_datetime',\n",
    "                               'imp_surcharge': 'improvement_surcharge',\n",
    "                               'vendor_id': 'vendorid',\n",
    "                               'rate_code' : 'ratecodeid',\n",
    "                               }, inplace = True)\n",
    "df_taxi_2015.rename(columns = {'pickup_datetime': 'tpep_pickup_datetime',\n",
    "                               'dropoff_datetime': 'tpep_dropoff_datetime',\n",
    "                               'imp_surcharge': 'improvement_surcharge',\n",
    "                               'vendor_id': 'vendorid'\n",
    "                              }, inplace = True)\n",
    "\n",
    "\n",
    "df_taxi=(df_taxi        \n",
    "         .append(df_taxi_2015)\n",
    "         .append(df_taxi_2014)\n",
    "         .append(df_taxi_2013)\n",
    "         .append(df_taxi_2012)\n",
    "         .append(df_taxi_2011)\n",
    "         .append(df_taxi_2009))\n",
    "\n",
    "df_taxi.rename(columns = {'tpep_dropoff_datetime': 'dropoff_datetime',\n",
    "                          'tpep_pickup_datetime': 'pickup_datetime',\n",
    "                         }, inplace = True)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check that all of the columns were alike we performed the following checking with each of the years. Changing the column names if it where needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#difference\n",
    "dif1=df_taxi.columns\n",
    "dif2=df_taxi_2015.columns\n",
    "print(np.setdiff1d(dif1,dif2))\n",
    "print(np.setdiff1d(dif2,dif1))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging all the year, we notice that the socrata sample was highly unbalanced. (there was even months that didn't appear.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve that a bigger sample ( in the order of several GB) were needed. So we decided continue only with year 2014, which the samples were always balance in contrast with the other years. For year distributions it was done a list with the number of rows present in each of the full datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with the dataset several changes have to be made. Some of those changes are done in the analysis but here we can find most of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, it is needed to change the datetime format to datetime so it can be extracted time features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE DATE TIME TO DATETIME\n",
    "\n",
    "df_taxi['pickup_datetime'] = pd.to_datetime(df_taxi['pickup_datetime'],format = '%Y-%m-%dT%H:%M')\n",
    "df_taxi['dropoff_datetime'] = pd.to_datetime(df_taxi['dropoff_datetime'],format = '%Y-%m-%dT%H:%M')\n",
    "df_taxi_all_columns = df_taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split date/time column\n",
    "df_taxi['pickup_datetime'] = pd.to_datetime(df_taxi['pickup_datetime'].copy())        \n",
    "#df_taxi['Year'] = df_taxi['pickup_datetime'].dt.year\n",
    "df_taxi['Month'] = df_taxi['pickup_datetime'].dt.month\n",
    "df_taxi['MonthDay'] = df_taxi['pickup_datetime'].dt.day\n",
    "df_taxi['DayOfWeek'] = df_taxi['pickup_datetime'].dt.dayofweek\n",
    "df_taxi['HourOfDay'] = df_taxi['pickup_datetime'].dt.hour\n",
    "df_taxi[\"HourOfWeek\"] = (df_taxi[\"DayOfWeek\"]*24 )+df_taxi[\"HourOfDay\"].astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lattitude and longitud is formated from strings to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE LONGITUTE AND LATITUDE TO FLOAT\n",
    "#df_taxi=df_taxi.iloc[:,[1,0,2,3,4,5,6]]\n",
    "df_taxi.rename(columns={\"pickup_latitude\": \"Lat\", \"pickup_longitude\": \"Lon\"},inplace=True)\n",
    "df_taxi['Lon']=df_taxi['Lon'].apply(lambda x: float(x))\n",
    "df_taxi['Lat']=df_taxi['Lat'].apply(lambda x: float(x))\n",
    "df_taxi['dropoff_longitude']=df_taxi['dropoff_longitude'].apply(lambda x: float(x))\n",
    "df_taxi['dropoff_latitude']=df_taxi['dropoff_latitude'].apply(lambda x: float(x))\n",
    "df_taxi['trip_distance']=df_taxi['trip_distance'].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the first analysis that we wanted to perform is regarding time distributions. In order to do that we show different distributions in the form of histograms mainly. Therefore, for using the same format we created a plot function. We used Histrograms over other figures, since they are clear way to show grouped distributions. Additionally, in this case most of the histogram are counts, giving a natural feeling of value sumed in piles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING BY HOUR OF THE DAY\n",
    "def Grouped_plot(x_taxi,Index_list,label_plot,version):\n",
    "    \n",
    "    if version == 1:\n",
    "        # set width of bars\n",
    "        barWidth = 0.8\n",
    "\n",
    "        # set heights of bars\n",
    "        bars1 = list(x_taxi)\n",
    "\n",
    "        # Make the plot\n",
    "\n",
    "        plt.figure(figsize = [12, 6])\n",
    "\n",
    "        plt.bar(range(1, len(bars1)+1),height = bars1, width=barWidth,\n",
    "                label='Taxi', alpha=0.5, align = 'center',color='yellow',edgecolor = \"black\")\n",
    "\n",
    "        plt.xlabel(label_plot)\n",
    "        plt.ylabel('Number of Pick Ups  ')\n",
    "        title_plot=label_plot+' PickUps distribution'\n",
    "        plt.title(title_plot)\n",
    "\n",
    "        # Add xticks on the middle of the group bars\n",
    "        plt.xlabel(label_plot, fontweight='bold')\n",
    "        plt.xticks([r + barWidth for r in range(len(bars1))], Index_list)\n",
    "        \n",
    "    else:\n",
    "        # set heights of bars\n",
    "        bars1 = list(x_taxi)      \n",
    "        \n",
    "        # Set position of bar on X axis\n",
    "        r1 = np.arange(len(bars1))\n",
    "\n",
    "\n",
    "        #plot\n",
    "        plt.figure(figsize = [12, 6])\n",
    "        title_plot=label_plot+' PickUps distribution'\n",
    "        plt.title(title_plot)\n",
    "        ax = plt.bar(r1, x_taxi,  label='Taxi', alpha=0.5);\n",
    "\n",
    "        plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "\n",
    "        # Create legend & Show graphic\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dayhour_taxi = df_taxi.groupby(\"HourOfDay\")[\"HourOfDay\"].count()\n",
    "df_dayhour_taxi= pd.DataFrame(df_dayhour_taxi/df_dayhour_taxi.sum())\n",
    "\n",
    "df_dayhour = pd.DataFrame({'Taxi':  df_dayhour_taxi['HourOfDay'].to_list()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grouped_plot(df_dayhour['Taxi'].values,\n",
    "             range(1, len(list(df_dayhour['Taxi'].values))+1),'Hour of the Day',1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PICK UPS BY WEEK DAY\n",
    "df_weekday_taxi = df_taxi.groupby(\"DayOfWeek\")[\"DayOfWeek\"].count()\n",
    "df_weekday_taxi=pd.DataFrame(df_weekday_taxi/df_weekday_taxi.sum())\n",
    "\n",
    "df_weekday = pd.DataFrame({'Taxi':  df_weekday_taxi['DayOfWeek'].to_list()},\n",
    "                       index=[ 'Monday', 'Tuesday', 'Wednesday', 'Thursday',\n",
    "                              'Friday', 'Saturday', 'Sunday' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Grouped_plot(df_weekday['Taxi'].values,\n",
    "             df_weekday.index.to_list(),'Week Days',1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONTHLY\n",
    "df_month_taxi = df_taxi.groupby(\"Month\")[\"Month\"].count()\n",
    "df_month_taxi=pd.DataFrame(df_month_taxi/df_month_taxi.sum())\n",
    "\n",
    "df_month = pd.DataFrame({'Taxi':  df_month_taxi['Month'].to_list()},\n",
    "                       index=[ 'January', 'February', 'March', 'April', 'May', 'June', 'July',\n",
    "                              'August', 'September', 'October', 'November', 'December'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated before the following list ares the different years of which we had the datasets and the number of rows that them contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista1=['2009','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020']\n",
    "lista2=[171,168,170,176,173,165,146,131,113,112,84,24.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Grouped_plot(lista2,\n",
    "             lista1,'Year',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will see in continuation, for hourly group of an entier week this plot is not as clean as in the previous one. Therefore, we found other way to show these distributions which is shown later, on this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekhour \n",
    "df_weekhour_taxi = df_taxi.groupby(\"HourOfWeek\")[\"HourOfWeek\"].count()\n",
    "df_weekhour_taxi=pd.DataFrame(df_weekhour_taxi/df_weekhour_taxi.sum())\n",
    "\n",
    "df_weekhour = pd.DataFrame({'Taxi':  df_weekhour_taxi['HourOfWeek'].to_list()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Grouped_plot(df_weekhour['Taxi'].values,\n",
    "           range(0, len(list(df_weekhour['Taxi'].values)), 24),'Week Hours',2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting analysis was to visualize how in peaks hours the average speed is reduced due to the congestion of the city, this is shown in the following figures, for which was needed to create a new variable \"average_speed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triptimeinhours(pickup_time, dropoff_time):\n",
    "        \n",
    "    return (dropoff_time - pickup_time).total_seconds()/3600 \n",
    "\n",
    "df_taxi['trip_hours'] = df_taxi.apply(lambda x: triptimeinhours(x['pickup_datetime'],x['dropoff_datetime']), axis=1)\n",
    "df_taxi['average_speed'] = df_taxi['trip_distance'].astype(float).divide(df_taxi['trip_hours'])\n",
    "df_taxi.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg = df_taxi.groupby(\"HourOfDay\")[\"average_speed\"].mean()\n",
    "df_avg= pd.DataFrame(df_avg/df_avg.sum())\n",
    "\n",
    "df_avg_spd = pd.DataFrame({'Taxi':  df_avg['average_speed'].to_list()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Grouped_plot(df_avg_spd['Taxi'].values,\n",
    "           range(1, len(list(df_avg_spd['Taxi'].values))+1),'Average speed hourly',1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking how change the average speed which was clear the factors that influence this, we wanted to explore what was the different factors that influence the duration of a trip. In order to that, a visual and direct way of visualize this influence was with a correlation matrix heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr = df_taxi.corr()\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(corr,vmax=1,square=True,annot=True,cmap='YlOrBr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure were not included in the website since they were chaotic, but they were meant to show the different trip duration for  each of the influencer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.pointplot(x='HourOfDay',y='trip_hours',data=df_taxi.dropna()\n",
    "              ,hue='DayOfWeek')\n",
    "plt.xlabel('pickup_hour',fontsize=16)\n",
    "plt.ylabel('trip duration',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.pointplot(x='HourOfDay',y='trip_hours',data=df_taxi.dropna()\n",
    "              ,hue='Month')\n",
    "plt.xlabel('pickup_hour',fontsize=16)\n",
    "plt.ylabel('trip duration',fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the covariance matrix and after some transformations of the features performed at the data engeneering part, we choose the following features for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Dataset=df_taxi[['HourOfDay','Month',\n",
    "                     'dropoff_longitude','dropoff_latitude',\n",
    "                 'Lon','Lat','trip_distance']].dropna()\n",
    "\n",
    "X=Dataset[['HourOfDay','Month',\n",
    "                     'dropoff_longitude','dropoff_latitude',\n",
    "                 'Lon','Lat']].to_numpy()\n",
    "y=Dataset['trip_distance'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do predictions of the time it takes to a taxi to arrive the destination given the previous parameters, we use random_forest algorithm. Which is easy to implement and is difficult to overfit. However, the results are not straight forward. This problem was competition in kaggle in 2017 with a price of 30000$. The results were better than expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "regr = RandomForestRegressor()\n",
    "regr.fit(X_train, y_train)\n",
    "y_hat=regr.predict(X_test)\n",
    "plt.figure(figsize = [12, 6])\n",
    "sns.scatterplot(x=y_hat, y=y_test,\n",
    "                color='yellow', edgecolors='black')\n",
    "plt.xlabel('Prediction', fontweight='bold')\n",
    "plt.ylabel('Real value', fontweight='bold')\n",
    "plt.title('Prediction vs Real value of trip distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following analysis we create an interactive plot for solve the problem with the hourly week distributions. In order to do that we used the library bokeh. At the beggining we used histograms again for each day. However, even with the optimal alpha parameter, the plot became chaotic when 3 or more days were shown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColorBar, ColumnDataSource\n",
    "from bokeh.palettes import Spectral6\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.transform import linear_cmap\n",
    "from bokeh.models import ColumnDataSource, FactorRange, Legend\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_notebook() # open the bokeh viz on the notebook.\n",
    "## it is a standard way to convert your df to bokeh\n",
    "source_taxi = ColumnDataSource(df_grouped_taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_taxi = figure(x_range=FactorRange(factors=list(map(str, (df_grouped_taxi.index+1).values.tolist()))), \n",
    "           plot_height=400, plot_width=800, title='Hourly Week Days Distribution Taxi',\n",
    "           x_axis_label='Hour of the Day', y_axis_label='Proportioned Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = palettes.Category20[len(weekday_list)]\n",
    "bar ={} # to store vbars\n",
    "items = []\n",
    "### here we will do a for loop:\n",
    "for indx,i in enumerate(weekday_list):\n",
    "    bar[i] = p_taxi.line(x='Hours', \n",
    "                    y=i, \n",
    "                    source=source_taxi, \n",
    "                    width=0.9,\n",
    "                    muted=True, \n",
    "                    muted_alpha=0.005,\n",
    "                    color=color[indx])\n",
    "    items.append((i, [bar[i]]))\n",
    "    \n",
    "legend = Legend(items=items)\n",
    "p_taxi.add_layout(legend, 'left')\n",
    "p_taxi.legend.click_policy = 'mute'\n",
    "p_taxi.legend.label_text_font_size='7pt'\n",
    "show(p_taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bokeh gave some problem at the moment of uploading it to the website. But after researching, we manage to do it in the following way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file('plot_taxi.html', mode='inline')\n",
    "save(p_taxi)\n",
    "html = file_html(p_taxi, CDN, \"my plot\")\n",
    "with open(\"./plot_taxi.html\",\"w+\") as f:\n",
    "    f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualize and analyse the geographical data, it was used several methods. The first one was a folium heatmap, However, this plot was not used for the website since, a better figure was achieve later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate folium San Francisco base map with Stamen toner tile \"\"\"\n",
    "def generateBaseMap(default_location=[40.767937,-73.982155], default_zoom_start=10):\n",
    "    base_map = folium.Map(location=default_location, control_scale=True, \n",
    "                          zoom_start=default_zoom_start, tiles=\"Stamen toner\")\n",
    "    return base_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creates array of coordinates to input folium map marker map\"\"\"\n",
    "def to_coordinates(row):\n",
    "    return [row[\"Lat\"],row[\"Lon\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi['Coor']=df_taxi.apply(to_coordinates,axis=1)\n",
    "data_HM_taxi=df_taxi[['Lat', 'Lon', 'Coor']].groupby(['Lat', 'Lon']).count().reset_index().values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_map_taxi = generateBaseMap()\n",
    "\n",
    "HeatMap(data=df_taxi['Coor'], \n",
    "        radius=10, max_zoom=12).add_to(base_map_taxi)\n",
    "base_map_taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next method for visualize the geodata was a scatter heatmap of the coordinates. This was the selected method for show the coordinates of pickups and dropoff. However, this process was done on plotly and using DTU HPC due to the computation of this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In order to plot directly the longitud and latitude data, it was needed to convert this distance regarding the curvature of the globe, in order to conver the distance we create the harvesine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#euclidian distances introduce errors. Therefore, we bin with haversine functions.\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we filter for the area of interest, and also for eliminate wrong data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# remove wrong data.\n",
    "#plt.hist(df['X'], bins='auto')\n",
    "#plt.hist(df['Y'], bins='auto')\n",
    "indexNames = df_taxi[(df_taxi['Lon'] < -74.08) |(df_taxi['Lon'] > -73.84) | (df_taxi['Lat'] > 40.86)| (df_taxi['Lat'] < 40.60)].index\n",
    "df_taxi_drop=df_taxi.drop(indexNames)\n",
    "#plt.hist(df['X'], bins='auto')\n",
    "#plt.hist(df['Y'], bins='auto')\n",
    "#bin estimation\n",
    "lat_min = df_taxi_drop['Lat'].min()\n",
    "lat_max = df_taxi_drop['Lat'].max()\n",
    "lon_min = df_taxi_drop['Lon'].min()\n",
    "lon_max = df_taxi_drop['Lon'].max()\n",
    "\n",
    "yharvesine=(10/2)*(haversine(lon_min, lat_min, lon_min, lat_max)+haversine(lon_max, lat_min, lon_max, lat_max))\n",
    "xharvesine=(10/2)*(haversine(lon_min, lat_max, lon_max, lat_max)+haversine(lon_min, lat_min, lon_max, lat_min))\n",
    "\n",
    "count, lon, lat = np.histogram2d(df_taxi_drop.Lat, df_taxi_drop.Lon, bins = [int(xharvesine),int(yharvesine)])\n",
    "\n",
    "#coordinates = df[['Lon', 'Lat']]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "ax= plt.imshow(count, cmap='magma',vmax=(500/(df_taxi_drop.shape[0]/df_taxi_drop.shape[0])), origin='lower') \n",
    "#vmax 500/4 - 4 times less observations after dropping\n",
    "\n",
    "plt.grid(b=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next piece of code is not used as an analysis and more like a wrap. We use a Heatmap in folium with time to show all the analysis that we have made previously in real time so the reader can get a more real experience of what is happening regarding what it was say in the previous analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_data_taxi = df_taxi[['Lat', 'Lon','HourOfWeek']]\n",
    "prep_data_taxi = prep_data_taxi.dropna(axis=0, subset=['Lat', 'Lon','HourOfWeek'])\n",
    "taxi_timeline = [[[row['Lat'],row['Lon']] for index, row in prep_data_taxi[prep_data_taxi['HourOfWeek'] == i].iterrows()] for i in range(0,166)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_map = folium.Map([40.767937,-73.982155], tiles='Stamen Toner', zoom_start=10)\n",
    "HeatMapWithTime(taxi_timeline, auto_play=True,\n",
    "                             radius=6).add_to(base_map)\n",
    "display(base_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_map.save(outfile= \"taxi_heatmap_time.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, the heatmap with time was impossible to upload to the website due it's weight, it was created a gif using selenium to save each photogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from IPython.display import IFrame\n",
    "\n",
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from PIL import Image \n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize map\n",
    "def embed_map(map, filename):\n",
    "    map.save(filename)\n",
    "    return IFrame(filename, width='100%', height='600px')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dow_dict = {0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df_taxi.DayOfWeek.min(), df_taxi.DayOfWeek.max()+1):\n",
    "    for j in range(df_taxi.HourOfDay.min(), df_taxi.HourOfDay.max()+1):    \n",
    "        \n",
    "        # Filter to include only data for each day of week and hour of day\n",
    "        df_geo = df_taxi.loc[(df_taxi.DayOfWeek==i) & (df_taxi.HourOfDay==j)][['Lat', 'Lon']].copy()\n",
    "\n",
    "        # Instantiate map object \n",
    "        map_5 = folium.Map(location=[40.75, -73.96], tiles='Stamen Toner', zoom_start=9)\n",
    "\n",
    "        # Plot heatmap\n",
    "        HeatMap(data=df_geo, radius=10).add_to(map_5)\n",
    "\n",
    "        # Get day of week string from dow_dict\n",
    "        d = dow_dict.get(i)\n",
    "        \n",
    "        # Add title to heatmap\n",
    "        title_html = f'''<h3 align=\"center\" style=\"font-size:20px\">\n",
    "                        <b>Taxi Pickups at {j}:00 on {d}: {len(df_geo)} rides</b></h3>\n",
    "                     '''\n",
    "        map_5.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "        #Save map\n",
    "        embed_map(map_5, f'./maps_png_pickup/{i}_{j}_heatmap.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
